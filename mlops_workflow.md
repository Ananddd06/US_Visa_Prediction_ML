# ğŸŒŸ **Project Workflow Guide** ğŸŒŸ

Welcome to the project! Here's your step-by-step guide to get started and understand the workflow.

---

## **Step 1: Initial Setup** ğŸ› ï¸

Let's kickstart the project with a solid foundation! ğŸ‰

### 1. **Initialize a Git Repository** ğŸ“

- Set up a Git repository to track changes and maintain version control. ğŸ§‘â€ğŸ’»
- Commit all initial files to ensure smooth collaboration.

### 2. **Core Files to Add** ğŸ“

- **`requirements.txt`**: List all the dependencies and libraries required for the project. ğŸ“¦
- **`setup.py`**: Contains the setup script for installing dependencies and configuring the project. ğŸ—ï¸
- **`README.md`**: A markdown file that provides an overview of the project, setup instructions, and usage guidelines. ğŸ“š

### 3. **Automating Directory and File Creation** ğŸ—‚ï¸

- Use a **template file** to define the necessary directory structure and files.
- Iterate over the list of directories and files using a loop:
  - Split the directory and file names with `os.path.split(data_dir, datafile)`.
  - Use `os.mkdir()` to create directories.
  - Check if the file exists and open it using `with open(file_name, "w")` if not.

---

## **Step 2: Setup Script** âš™ï¸

### 1. **Package Installation** ğŸ§‘â€ğŸ”§

- Write a function to install all dependencies listed in the `requirements.txt` file.

### 2. **Setup Function** ğŸ”§

- Include key details such as:
  - Author name âœï¸
  - Author email ğŸ“§
  - Project name ğŸ“œ
- Call the function to install the required packages.

---

## **Step 3: Exception Handling and Logging** ğŸ”’

### 1. **Exception Handling** ğŸš¨

- Handle exceptions for:
  - Missing files ğŸ“‚: Raise a `FileNotFoundError` if a specified file is not found.
  - Missing directories ğŸ“: Raise an appropriate error if directories are not found or accessible.

### 2. **Logging** ğŸ“–

- Implement a logging system to:
  - Track the workflow ğŸ”
  - Log timestamps and actions for better traceability ğŸ•’
- Use log files to debug issues efficiently. ğŸ

---

## **Step 4: Data Exploration and Model Building** ğŸ“Š

Create a folder named **`Notebook`** for all analysis and modeling tasks. This will include:

### 1. **Attach the Dataset** ğŸ“‚

- Include the `.csv` file for the project.

### 2. **Perform Exploratory Data Analysis (EDA)** ğŸ”

- Understand the dataset structure and uncover insights.

### 3. **Feature Engineering** ğŸ§ 

- Engineer new features to improve model performance.

### 4. **Model Training and Hyperparameter Tuning** ğŸ¤–

- Train models and tune hyperparameters to achieve the best results.

---

## **Step 5: MongoDB Database Connection** ğŸŒ

In this step, we connect to the MongoDB database.

### 1. **MongoDB URL Configuration** ğŸ—ºï¸

- In the **config** file, under the `US Visa` folder, make the MongoDB URL connection.
- Check if a client connection exists. If not, fetch the URL and connect to MongoDB.
- Convert the data from the MongoDB collection into a pandas dataframe and then to a dictionary format for further processing.

---

## **Step 6: Convert Data Table to CSV File** ğŸ’¾

Create a **data_access** folder where we connect to MongoDB and export the data to a `.csv` file.

### 1. **Function Creation** ğŸ› ï¸

- Access the MongoDB database, extract the required data, and save it as a CSV file. This file will be used for training and testing data.

---

## **Step 7: Workflow Steps** ğŸ”„

The overall project follows this workflow:

1. **Entity** ğŸ¢
2. **Artifact** ğŸ—ï¸
3. **Config** âš™ï¸
4. **Components** ğŸ”Œ
5. **Training Pipeline** ğŸ‹ï¸â€â™€ï¸

---

## **Step 8: Define Constants** ğŸ“

In the **constants** folder, define all necessary paths and database configurations.

```python
import os
from datetime import date

DATABASE_NAME = "US_VISA"
COLLECTION_NAME = "visa_data"
MONGO_DB_URL = "MONGODB_URL"

PIPELINE_NAME: str = "usvisa"
ARTIFACT_DIR: str = 'artifact'

MODEL_FILE_NAME = "model.pkl"
TRAIN_FILE_NAME: str = "train.csv"
TEST_FILE_NAME: str = "test.csv"
FILE_NAME: str = "usvisa.csv"
MODEL_FILE_NAME = "model.pkl"
TARGET_COLUMN = "case_status"
CURRENT_YEAR = date.today().year
PREPROCESSING_OBJECT_FILE_NAME = "preprocessing.pkl"
SCHEMA_FILE_PATH = os.path.join("config", "schema.yaml")

```

## **Step 9: Define Data Ingestion Path in Config** ğŸ”„

### 1. **Data Ingestion Path Configuration** ğŸ›¤ï¸

- Add a key in the **config** file to store the path where data ingestion files (`train.csv`, `test.csv`) will be saved.
- Define the paths and file names for the ingested data to align with the project structure.

---

## **Step 10: Data Ingestion Workflow** ğŸ“ˆ

The **DataIngestion** class handles data extraction, transformation, and storage.

### 1. **Feature Store Function** ğŸª

- In the **DataIngestion** class, create a function to fetch MongoDB data, convert it to CSV, and store it in the feature store.
- Interact with MongoDB, fetch required data, and save it in `.csv` format for further processing.

### 2. **Train-Test Split** ğŸ”ª

- Implement functionality to split the data into training and testing sets.
- The split data will be saved in the **ingested** folder under `train.csv` and `test.csv` files, referenced dynamically from the **config** file.

### 3. **Initiate Data Ingestion Workflow** ğŸš€

- Use the **DataIngestion** class to execute the ingestion process and save the resulting data (like `train.csv` and `test.csv`) in the **artifact** folder.

---

## **Step 11: Update Training Pipeline** âš™ï¸

1. **TrainingPipeline Class** ğŸ’¼

   - In the **TrainingPipeline** class, import the necessary config and call the **DataIngestion** class to set up the ingestion process.

2. **Start Function** ğŸ

   - Create a function named **start** to initiate the data ingestion process and call the **DataIngestion** class to load data.

3. **Run Pipeline Function** ğŸƒâ€â™‚ï¸
   - The **run_pipeline** function will initiate the **start** method from **DataIngestion**, launching the entire pipeline.

---

## **Step 12: Data Validation Workflow** âœ…

Data validation ensures that the incoming data is clean, error-free, and adheres to the defined schema.

### 1. **Config and Artifact Updates** ğŸ“‚

- Add necessary configuration to the **config** file for storing validation rules.
- Ensure that the artifact files for storing the validation results are properly created and logged.

### 2. **Schema Validation** ğŸ“Š

- Use a schema validation library like **Cerberus** or **pydantic** to validate incoming data.
- Define rules to ensure the data matches the required structure (column names, data types, ranges, etc.).
- Raise errors for invalid data to prevent further processing.

### 3. **Log Validation Results** ğŸ“

- Store validation results in a separate **log file** for debugging and record-keeping purposes.
- Track the number of valid and invalid entries.

---

## **Step 13: Model Training and Hyperparameter Tuning** ğŸ¤–

Now that the data has been ingested, cleaned, and validated, it's time to move to the next phase: **Model Training**.

### 1. **Train Models** ğŸ“š

- Use a variety of algorithms (e.g., logistic regression, decision trees, random forests) to train the model on the processed data.
- Ensure that the model selection aligns with the problem (classification, regression, etc.).

### 2. **Hyperparameter Tuning** ğŸ”§

- Apply techniques such as grid search or random search to tune the hyperparameters of the model.
- Use cross-validation to evaluate the performance of the model during training.

### 3. **Evaluate Model Performance** ğŸ“ˆ

- Track important metrics like accuracy, precision, recall, F1-score, etc., to assess the performance of the model.
- Generate confusion matrices and ROC curves to evaluate the model visually.

### 4. **Save the Best Model** ğŸ’¾

- Once the model is trained and optimized, save it in the **artifact** folder using `joblib` or `pickle`.
- The best model will be used for future predictions.

---

## **Next Steps** ğŸš€

After completing these stages, continue with the following steps in the project lifecycle:

1. **Model Deployment** ğŸš€
   - Deploy the model in a production environment using Flask, FastAPI, or a cloud-based solution.
2. **Monitoring and Maintenance** ğŸ”„

   - Set up monitoring for the deployed model to track its performance over time.
   - Periodically retrain the model with fresh data to keep it up to date.

3. **Integration with APIs** ğŸŒ
   - Integrate the model with external systems, APIs, or user interfaces for real-time predictions and insights.

---
